{"cells":[{"metadata":{"_uuid":"7a2447f9c70eb98a2f3bbba16cbf5393de5cce61"},"cell_type":"markdown","source":"# Predicting Customer Satisfaction - Ecommerce Data\nOlist has released a dataset of 100k orders made between 2016 and 2018. Lets create a model to predict what's the score a customer will give for an order."},{"metadata":{"_uuid":"a07553dca2b6952fa380a15c41cc4ca795e72d6a"},"cell_type":"markdown","source":"# 1. Exploratory Data Analysis\nSome **EDAs (Exploratory Data Analysis)** were already made by other users and are publicly available at the dataset's kernels. That's why we're going to skip much of the EDA and jump into the problem[](http://). We recommend the following EDAs:\n* [E-Commerce Exploratory Analysis](https://www.kaggle.com/jsaguiar/e-commerce-exploratory-analysis) by [Aguiar](https://www.kaggle.com/jsaguiar)\n* [Data Cleaning, Viz and Stat Analysis on e-com](https://www.kaggle.com/goldendime/data-cleaning-viz-and-stat-analysis-on-e-com) by [Azim Salikhov](https://www.kaggle.com/goldendime)\n\nThose analysis help us understand what is happening with data. After we are confortable with it, and confident of its value we may start working on bigger problems. "},{"metadata":{"trusted":true,"_uuid":"9f9f170dcada59322448b96a6108579c4d822c14"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime as dt\n\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\norders = pd.read_csv('../input/olist_public_dataset_v2.csv')\n\n# converting to datetime\norders['order_purchase_timestamp'] = pd.to_datetime(orders.order_purchase_timestamp)\norders['order_aproved_at'] = pd.to_datetime(orders.order_aproved_at).dt.date  \norders['order_estimated_delivery_date'] = pd.to_datetime(orders.order_estimated_delivery_date).dt.date  \norders['order_delivered_customer_date'] = pd.to_datetime(orders.order_delivered_customer_date).dt.date  \n\n# get translations for category names\ntranslation = pd.read_csv('../input/product_category_name_translation.csv')\norders = orders.merge(translation, on='product_category_name').drop('product_category_name', axis=1)\n\norders.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"733d82df24c2a5d9b6afc21fd0b0c301080b6b44"},"cell_type":"markdown","source":"# 2. Defining the Problem\nLet's say your manager asked you: \n\n**\"What is the probable score that we getting from customers?\"**\n\nOur problem is to find a way to estimate, based on data about the product and order, what will be the customer review score.\n\n# 3. The hypothesis\nOur main hypothesis is that the product and how the order was fulfilled might influence the customer review score. Keep in mind that each feature we create is a new hypothesis we are testing."},{"metadata":{"_uuid":"33a54498449c7fdfabd616bb45c0f38cbb97beef"},"cell_type":"markdown","source":"# 4. Designing an Experiment\nTo answer that question we must implement collect data from each order up to delivery phase. With that, we should implement a model that estimates what will be the score given by the customer at the review phase.\n\n![frame the problem](https://i.imgur.com/MTLzY55.png)\n\n####  How would you frame this problem? \nIf you would try a different approach, please leave a comment or write a kernel!\n"},{"metadata":{"_uuid":"2e832cf294b02b97d24a9d372af8acaae3589413"},"cell_type":"markdown","source":"# a. Drop columns\nSome columns have information about the review given by a customer (review_coment_message, review_creation_date, etc), but we don't want to use that. Our experiment assumes we don't have any information about the review, so we need to predict the score before a customer writes it. There are also some columns that are unninformative to predict the customer satisfaction."},{"metadata":{"trusted":true,"_uuid":"42d79f6724dca2dce1ea0d8c073aa30af6e2a4d5"},"cell_type":"code","source":"orders = orders[['order_status', 'order_products_value',\n                 'order_freight_value', 'order_items_qty', 'order_sellers_qty',\n                 'order_purchase_timestamp', 'order_aproved_at', 'order_estimated_delivery_date', \n                 'order_delivered_customer_date', 'customer_state', \n                 'product_category_name_english', 'product_name_lenght', 'product_description_lenght', \n                 'product_photos_qty', 'review_score']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdca56e83efda75bcc04834b8942d8268cea7e9a"},"cell_type":"markdown","source":"# b. Spliting the Dataset\nIt is important that we split our data at the very beginning of our analysis. Doing that after might introduce some unwanted bias. \n\n> To split correctly, lets first see how classes are distributed over the full dataset."},{"metadata":{"trusted":true,"_uuid":"2678292156690f532118c3d8ddb5c4cd1882d315"},"cell_type":"code","source":"# We keep the same proportion of classes\norders['review_score'].value_counts() / len(orders['review_score'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac6036f820f45484ba6ab7b50acbab8e5cfed8e2"},"cell_type":"markdown","source":"## Simple split\nLets first try a simple random split and lets see if the proportions are kept equal."},{"metadata":{"trusted":true,"_uuid":"0c9afb0be2e3c3f0ab6000e5eafc7b4fcd4313eb"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# split\ntrain_set, test_set = train_test_split(orders, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1de99a1ec657bee59a4ba2e1038de2c4b9f92b15"},"cell_type":"code","source":"test_set['review_score'].value_counts() / len(test_set['review_score'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"baa1483fff0ffd9f6a393010c1000eeba774f41c"},"cell_type":"markdown","source":"We see there is some difference between the proportion of each class compared to the original dataset.\n\n## Stratified Split\nNow lets do a stratified shuffle split and compare to the full dataset again."},{"metadata":{"trusted":true,"_uuid":"4bd5d98b0e89049b8bff2274495ee0f576123ffb"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\n# Stratified Split\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(orders, orders['review_score']):\n    strat_train_set = orders.loc[train_index]\n    strat_test_set = orders.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3c11ffd8827b893dfabaf3450aeafe26d3f29d7"},"cell_type":"code","source":"strat_train_set['review_score'].value_counts() / len(strat_train_set['review_score'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7e4a8a979b915ee0970044241a571830277143b"},"cell_type":"markdown","source":"By doing a stratified split we keep the same proportion between classes. This split better represent the original data and will possibli reduce any bias. "},{"metadata":{"_uuid":"e5ce440c5aa169bf2b8549bc36d89ddfbcd717c7"},"cell_type":"markdown","source":"# c. Separate Labels From Features\nWe don't wanto to apply any transformation to the labels (review_score). To avoid that we just create a separate serie with labels, and drop the target column from features dataset."},{"metadata":{"trusted":true,"_uuid":"079f3152d8bc795cbf8d5f68c5eefbfc273c4bd1"},"cell_type":"code","source":"orders_features = strat_train_set.drop('review_score', axis=1)\norders_labels = strat_train_set['review_score'].copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbb09803d93addd6c893584d83ca21fbd1fff30d"},"cell_type":"markdown","source":"# d. Feature Engineering\nIf we see the original data there aren't many columns that are correlated to target."},{"metadata":{"trusted":true,"_uuid":"a60da0fdce1feba66cea2c6c66394e313fad20de"},"cell_type":"code","source":"corr_matrix = strat_train_set.corr()\ncorr_matrix['review_score'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"92f02ddbbffe45a86adb87ccaaa703d878ba618d"},"cell_type":"markdown","source":"It's clear that we have to create more informative features to model this problem.\n\n## Features Hypotesis\n\n#### Working Days Estimated Delivery Time\nGets the days between order approval and estimated delivery date. A customer might be unsatisfied if he is told that the estimated time is big.\n\n#### Working Days Actual Delivery Time\nGets the days between order approval and delivered customer date. A customer might be more satisfied if he gets the product faster.\n\n#### Working Days Delivery Time Delta\nThe difference between the actual and estimated date.  If negative was delivered early, if positive was delivered late. A customer might be more satisfied if the order arrives sooner than expected, or unhappy if he receives after the deadline\n\n#### Is Late\nBinary variable indicating if the order was delivered after the estimated date.\n\n#### Average Product Value\nCheaper products might have lower quality, leaving customers unhappy.\n\n#### Total Order Value\nIf a customer expends more, he might expect a better order fulfilment.\n\n#### Order Freight Ratio\nIf a customer pays more for freight, he might expect a better service.\n\n#### Purchase Day of Week\nDoes it affect how happy are the customers?"},{"metadata":{"trusted":true,"_uuid":"1d6e301394142bd1bcf07457443448ed2c672832"},"cell_type":"code","source":"#plt.figure(figsize=(20,5))\n#sns.heatmap(corr_matrix)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"6bbf95556239df41e3e1ae83aff6c41859daa302"},"cell_type":"code","source":"# To consider Brazilian calendar and hollidays\n!pip install workalendar\nfrom workalendar.america import Brazil\ncal = Brazil()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a34d93bec720451fcf0677d6a9faee29eb001c6"},"cell_type":"markdown","source":"## Creating a Custom Transformer for FeatEng\nWe need to guarantee that we are apply exactly the same transformation to new/unseen data. To do that we will create custom transformers using scikit-learn BaseEstimator.\n\nThis first custom transformer will do the feature engineering that we just described earlier."},{"metadata":{"trusted":true,"_uuid":"95d67673cfa37dbb1869c5a62de43a5265da63e4"},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass AttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass    \n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        df = X.copy()\n        \n        # Calculate the estimated delivery time and actual delivery time in working days. \n        # This would allow us to exclude hollidays that could influence delivery times.\n        # If the order_delivered_customer_date is null, it returns 0.\n        df['wd_estimated_delivery_time'] = df.apply(lambda x: cal.get_working_days_delta(x.order_aproved_at, \n                                                                                      x.order_estimated_delivery_date), axis=1)\n        df['wd_actual_delivery_time'] = df.apply(lambda x: cal.get_working_days_delta(x.order_aproved_at, \n                                                                                   x.order_delivered_customer_date), axis=1)\n\n        # Calculate the time between the actual and estimated delivery date. If negative was delivered early, if positive was delivered late.\n        df['wd_delivery_time_delta'] = df.wd_actual_delivery_time - df.wd_estimated_delivery_time\n\n\n        # Calculate the time between the actual and estimated delivery date. If negative was delivered early, if positive was delivered late.\n        df['is_late'] = df.order_delivered_customer_date > df.order_estimated_delivery_date\n        \n        # Calculate the average product value.\n        df['average_product_value'] = df.order_products_value / df.order_items_qty\n\n        # Calculate the total order value\n        df['total_order_value'] = df.order_products_value + df.order_freight_value\n        \n        # Calculate the order freight ratio.\n        df['order_freight_ratio'] = df.order_freight_value / df.order_products_value\n        \n        # Calculate the order freight ratio.\n        df['purchase_dayofweek'] = df.order_purchase_timestamp.dt.dayofweek\n                       \n        # With that we can remove the timestamps from the dataset\n        cols2drop = ['order_purchase_timestamp', 'order_aproved_at', 'order_estimated_delivery_date', \n                     'order_delivered_customer_date']\n        df.drop(cols2drop, axis=1, inplace=True)\n        \n        return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"746fea0aeb43ce3957cf27ab6921b1cc0cf3292b"},"cell_type":"markdown","source":"### New Features - Working Days\nAnalysing the dataframe we see that the new features were succesfully created."},{"metadata":{"trusted":true,"_uuid":"1874b250373bbee28a8dc6cea4e80e5b29cc4d69"},"cell_type":"code","source":"# Executing the estimator we just created\nattr_adder = AttributesAdder()\nfeat_eng = attr_adder.transform(strat_train_set)\nfeat_eng.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ad62d52c592636fca1c32f55bfc3fcfb5358d8c"},"cell_type":"markdown","source":"### Correlation\nWhat is the correlation of the features we have just created with the review score?"},{"metadata":{"trusted":true,"_uuid":"63cd97253a7af0bec9cf96691df54f93751c1ca2"},"cell_type":"code","source":"corr_matrix = feat_eng.corr()\ncorr_matrix['review_score'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd96eee399b4782cbf6312f595bfd0d87e916e99"},"cell_type":"markdown","source":"Looks ok, there aren't any strong correlation. But it is clear that if a customer will give a lower score if he gets an order after the estimated date. \n\n## Any missing values?\nLet's see if there are any missing values."},{"metadata":{"trusted":true,"_uuid":"ad7867fbde1ff79a87d1f431e05c1b6f9e9bb03a"},"cell_type":"code","source":"feat_eng.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"080af983075f2892d540a08860748e2f582d05dc"},"cell_type":"markdown","source":"Great! No missing values after this transformation!\n\n# e. Data Viz\nNow let's visually explore this dataset a little bit!"},{"metadata":{"_uuid":"b6ed898668040e1eca5bc2dad616494e376ce237"},"cell_type":"markdown","source":"# f. Dealing with Categorical and Numerical Attributes\nThe way we handle categorical data is very different from the transformations needed for numerical features. We will create a transformer to select only categorical or numerical features for processing."},{"metadata":{"trusted":true,"_uuid":"ba534d66cea2510af4dc51176caeae2d97e68f80"},"cell_type":"code","source":"# selecting the numerical and text attributes\ncat_attribs = ['order_status', 'customer_state', 'product_category_name_english']\nnum_attribs = orders_features.drop(cat_attribs, axis=1).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae5e5212222181ab84dacece728c87761c421785"},"cell_type":"code","source":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X[self.attribute_names]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cc4189896d5731aedfa94054bac2402a5687d2f"},"cell_type":"markdown","source":"## Numerical Attributes\nCreating pipelines to handle unseen data"},{"metadata":{"trusted":true,"_uuid":"d24ec4a6163c6cb2a91049cd6568aebd17d409d9"},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# for now we wont work with categorical data. Planning to add it on next releases\nnum_pipeline = Pipeline([('selector', DataFrameSelector(num_attribs)),\n                         ('attribs_adder', AttributesAdder()),\n                         ('std_scaller', StandardScaler())\n                        ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af2f1eb62ae9bf146ef2d50becc8407a1291366b"},"cell_type":"code","source":"# lets see how the resulting data looks like\norders_features_prepared = num_pipeline.fit_transform(orders_features)\norders_features_prepared","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aff98f01e2318f3a84f4267a652b32cf32da9e11"},"cell_type":"markdown","source":"# g. Selecting a Model\nStart simple.\n\n## Linear Regression"},{"metadata":{"trusted":true,"_uuid":"85c9d31f695f121b43ce2a0348ac1d0a23af0c66"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(orders_features_prepared, orders_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7ef87d5770e74ee2e6e88aef873c1528868c4ce"},"cell_type":"code","source":"some_data = orders_features.iloc[:8]\nsome_labels = orders_labels.iloc[:8]\nsome_data_prepared = num_pipeline.transform(some_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"217172f7a04ee0cc66ee72af4140a02d877999c3"},"cell_type":"code","source":"print('Predicted: {} \\n Labels: {}'.format(list(lin_reg.predict(some_data_prepared)), list(some_labels.values)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4ad6c4408d3040fc0b3007a844d2d259f52acba"},"cell_type":"markdown","source":"Looks like we are not even close to predicting the right values. Lets see whats the root mean squared error."},{"metadata":{"trusted":true,"_uuid":"6c3beffcc45d7f2738d0c0ab8e3544fd6235d1aa"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\npredictions = lin_reg.predict(orders_features_prepared)\nlin_mse = mean_squared_error(orders_labels, predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"f46e28de068384f83cd86e6771414d734ad603ca"},"cell_type":"markdown","source":"A typical prediction error of about 1.25 is not at all satisfying when we are trying to predict values that range from 1 to 5.  So let's try a different model."},{"metadata":{"trusted":true,"_uuid":"4a2d5807462bd9d8f7563db2399e1ccb95212f79"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor()\nforest_reg.fit(orders_features_prepared, orders_labels)\n\npredictions = forest_reg.predict(orders_features_prepared)\nforest_mse = mean_squared_error(orders_labels, predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93c9248d48cfeb91564f55a5e71bcb99bce73aaf"},"cell_type":"markdown","source":"Much better! We got a typical error of 0.53 with Random Forest. Looks like it's a good algorithm! Let's see some examples of predictions."},{"metadata":{"trusted":true,"_uuid":"b6f768a4bd3ca40f0ed279a0cb88e0b097ecf770"},"cell_type":"code","source":"print('Predicted: {} \\n Labels: {}'.format(list(forest_reg.predict(some_data_prepared)), list(some_labels.values)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fbe8d1babc6d5bb147c7234e782fb2a5bbc1db1"},"cell_type":"markdown","source":"# Next steps\n\n1. Cross validation\n2. Grid search\n3. Full pipeline - transform and predict data\n4. Validation on test set\n5. Constructing a conclusion"},{"metadata":{"trusted":true,"_uuid":"f3edb75777b9e7bddbaf407e125f9952f531823a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}